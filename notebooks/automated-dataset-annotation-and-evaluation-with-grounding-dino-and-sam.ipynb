{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# Automated Dataset Generation with Grounding DINO + Segment Anything Model (SAM)\n",
        "\n",
        "---\n",
        "\n",
        "In this tutorial, you will learn how to automatically annotate your images using two groundbreaking models - [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) and [Segment Anything Model (SAM)](https://github.com/facebookresearch/segment-anything). You can then use this dataset to train a real-time object detection or instance segmentation model. Annotation of images using polygons in the traditional way is extremely time-consuming and expensive. With Grounding DINO and SAM, initial annotation takes only a few minutes and your work is reduced to manual verification of obtained labels.\n",
        "\n",
        "![auto-annotation-with-grounded-sam](https://media.roboflow.com/notebooks/examples/auto-annotation-with-grounded-sam.png)\n",
        "\n",
        "*Figure showing the effects of automatic annotation. The raw image on the left; Object detection annotations obtained with Grounding DINO in the middle; Instance segmentation annotations obtained with Grounding DINO + SAM on the right.*\n",
        "\n",
        "## Complementary Materials\n",
        "\n",
        "---\n",
        "\n",
        "Want to learn more? ü§ì We have prepared separate tutorials where you can learn how to use Grounding DINO and Segment Anything Model (SAM). You will find all the necessary links below.\n",
        "\n",
        "- Segment Anything Model\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/segment-anything) [![arXiv](https://img.shields.io/badge/arXiv-2304.02643-b31b1b.svg)](https://arxiv.org/abs/2304.02643) [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/D-D6ZmadzPE) [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-use-segment-anything-model-sam)\n",
        "\n",
        "- Grounding DINO\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/IDEA-Research/GroundingDINO) [![arXiv](https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg)](https://arxiv.org/abs/2303.05499) [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/cMa77r3YrDk) [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/grounding-dino-zero-shot-object-detection)\n",
        "\n",
        "## Pro Tip: Use GPU Acceleration\n",
        "\n",
        "If you are running this notebook in Google Colab, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times.\n",
        "\n",
        "## Steps in this Tutorial\n",
        "\n",
        "In this tutorial, we are going to cover:\n",
        "\n",
        "- Before you start\n",
        "- Install Grounding DINO and Segment Anything Model\n",
        "- Load models\n",
        "- Download Example Data\n",
        "- Single Image Mask Auto Annotation\n",
        "- Full Dataset Mask Auto Annotation\n",
        "- Convert Object Detection to Instance Segmentation Dataset\n",
        "\n",
        "## Let's begin! üî•"
      ],
      "metadata": {
        "id": "i0DY61Y3kEqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "lhpqQ3AVYQh7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3VXMWUdVy4M",
        "outputId": "776f7192-33f6-468d-fd85-131f842a11a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 22 05:26:12 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."
      ],
      "metadata": {
        "id": "AIcGNEx3YmKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ipjkvtAYSiR",
        "outputId": "fa506c73-0f94-4124-89c0-e057ce32ad28"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOME: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Grounding DINO and Segment Anything Model\n",
        "\n",
        "Our project will use two groundbreaking designs - [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) - for zero-shot detection and [Segment Anything Model (SAM)](https://github.com/facebookresearch/segment-anything) - for converting boxes into segmentations. We have to install them first.\n"
      ],
      "metadata": {
        "id": "s3VuH-MxY5ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd {HOME}/GroundingDINO\n",
        "!git checkout -q 57535c5a79791cb76e36fdb64975271354f10251\n",
        "!pip install -q -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnlQNpMpwaP3",
        "outputId": "6b5b9731-85b1-4654-db77-d95fa85a6116"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'GroundingDINO'...\n",
            "remote: Enumerating objects: 463, done.\u001b[K\n",
            "remote: Counting objects: 100% (240/240), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 463 (delta 176), reused 137 (delta 137), pack-reused 223 (from 1)\u001b[K\n",
            "Receiving objects: 100% (463/463), 12.87 MiB | 40.31 MiB/s, done.\n",
            "Resolving deltas: 100% (241/241), done.\n",
            "/content/GroundingDINO\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h    \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "    \n",
            "    \u001b[31m√ó\u001b[0m \u001b[32mpython setup.py develop\u001b[0m did not run successfully.\n",
            "    \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "    \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "    \n",
            "    \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m√ó\u001b[0m \u001b[32mpython setup.py develop\u001b[0m did not run successfully.\n",
            "\u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "\n",
        "import sys\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGwLQ7mBw9WJ",
        "outputId": "06ec4300-50cd-4006-b523-d9fca1c36def"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-xnnapjtc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-xnnapjtc\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment_anything\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment_anything: filename=segment_anything-1.0-py3-none-any.whl size=36592 sha256=7860c20a267137ff5f15cdde069f17d12ba48d7f242a6b83dd1369cfac0ababd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-10rwx3zh/wheels/15/d7/bd/05f5f23b7dcbe70cbc6783b06f12143b0cf1a5da5c7b52dcc5\n",
            "Successfully built segment_anything\n",
            "Installing collected packages: segment_anything\n",
            "Successfully installed segment_anything-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** To glue all the elements of our demo together we will use the [`supervision`](https://github.com/roboflow/supervision) pip package, which will help us **process, filter and visualize our detections as well as to save our dataset**. A lower version of the `supervision` was installed with Grounding DINO. However, in this demo we need the functionality introduced in the latest versions. Therefore, we uninstall the current `supervsion` version and install version `0.6.0`.\n",
        "\n"
      ],
      "metadata": {
        "id": "vrK8geyHt6JV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y supervision\n",
        "!pip install -q supervision==0.6.0\n",
        "\n",
        "import supervision as sv\n",
        "print(sv.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxU6_aWCNSFq",
        "outputId": "a5c778f5-90a0-4a9b-83c4-e7318fd66b27"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: supervision 0.4.0\n",
            "Uninstalling supervision-0.4.0:\n",
            "  Successfully uninstalled supervision-0.4.0\n",
            "0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** At the end of the tutorial we will upload our annotations to [Roboflow](roboflow.com). To automate this process with the API, let's install the `roboflow` pip package."
      ],
      "metadata": {
        "id": "U08MXHCEluJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q roboflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QevJDQsuu492",
        "outputId": "074df1d4-b3b1-4d14-bb76-23a72efb4423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/85.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Grounding DINO Model Weights\n",
        "\n",
        "To run Grounding DINO we need two files - configuration and model weights. The configuration file is part of the [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) repository, which we have already cloned. The weights file, on the other hand, we need to download. We write the paths to both files to the `GROUNDING_DINO_CONFIG_PATH` and `GROUNDING_DINO_CHECKPOINT_PATH` variables and verify if the paths are correct and the files exist on disk."
      ],
      "metadata": {
        "id": "GGV-pHiDgZs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "GROUNDING_DINO_CONFIG_PATH = os.path.join(HOME, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
        "print(GROUNDING_DINO_CONFIG_PATH, \"; exist:\", os.path.isfile(GROUNDING_DINO_CONFIG_PATH))"
      ],
      "metadata": {
        "id": "qtMIfEt4gdUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!mkdir -p {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "\n",
        "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth"
      ],
      "metadata": {
        "id": "p-Eoargcg3Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "GROUNDING_DINO_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"groundingdino_swint_ogc.pth\")\n",
        "print(GROUNDING_DINO_CHECKPOINT_PATH, \"; exist:\", os.path.isfile(GROUNDING_DINO_CHECKPOINT_PATH))"
      ],
      "metadata": {
        "id": "BVkviPClh0Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Segment Anything Model (SAM) Weights\n",
        "\n",
        "As with Grounding DINO, in order to run SAM we need a weights file, which we must first download. We write the path to local weight file to `SAM_CHECKPOINT_PATH` variable and verify if the path is correct and the file exist on disk."
      ],
      "metadata": {
        "id": "hJqh1YOwdahU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!mkdir -p {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ],
      "metadata": {
        "id": "XUEOO4bXdFJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "SAM_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
        "print(SAM_CHECKPOINT_PATH, \"; exist:\", os.path.isfile(SAM_CHECKPOINT_PATH))"
      ],
      "metadata": {
        "id": "ZsNiAQqTdM2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load models"
      ],
      "metadata": {
        "id": "ByAcxA43a484"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "xJzLz7oUZ0e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Grounding DINO Model"
      ],
      "metadata": {
        "id": "98JPP_icc5hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}/GroundingDINO\n",
        "\n",
        "from groundingdino.util.inference import Model\n",
        "\n",
        "grounding_dino_model = Model(model_config_path=GROUNDING_DINO_CONFIG_PATH, model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH)"
      ],
      "metadata": {
        "id": "odl3HAdWc5Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Segment Anything Model (SAM)"
      ],
      "metadata": {
        "id": "DBTCh84cczrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAM_ENCODER_VERSION = \"vit_h\""
      ],
      "metadata": {
        "id": "p23VWJ2adrK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH).to(device=DEVICE)\n",
        "sam_predictor = SamPredictor(sam)"
      ],
      "metadata": {
        "id": "-PBgnrmgcLSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Example Data\n",
        "\n",
        "Let's download few example images. Feel free to replace my images with yours. All you have to do is upload them to the `{HOME}/data` directory. If you're looking for data, take a peek at [Roboflow Universe](https://universe.roboflow.com/)! You're sure to find something interesting."
      ],
      "metadata": {
        "id": "9GaaZzF0jseo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f\"{HOME}/data\""
      ],
      "metadata": {
        "id": "3L6S4MeLLTfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!mkdir {HOME}/data\n",
        "%cd {HOME}/data\n",
        "\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-2.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-3.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-4.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-5.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-6.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-7.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-8.jpeg"
      ],
      "metadata": {
        "id": "XB-oJV2Gj6pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Image Mask Auto Annotation\n",
        "\n",
        "Before we automatically annotate the entire dataset let's focus for a moment on a single image."
      ],
      "metadata": {
        "id": "1euQGJ3ZG32x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_IMAGE_PATH = f\"{HOME}/data/dog-3.jpeg\"\n",
        "CLASSES = ['car', 'dog', 'person', 'nose', 'chair', 'shoe', 'ear']\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25"
      ],
      "metadata": {
        "id": "gCtHvuKoGa5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-Shot Object Detection with Grounding DINO"
      ],
      "metadata": {
        "id": "nrLU7SWFdGcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** To get better Grounding DINO detection we will leverage a bit of prompt engineering using `enhance_class_name` function defined below. üëá You can learn more from our [Grounding DINO tutorial](https://blog.roboflow.com/grounding-dino-zero-shot-object-detection/)."
      ],
      "metadata": {
        "id": "8Ll-gRnqvyOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "def enhance_class_name(class_names: List[str]) -> List[str]:\n",
        "    return [\n",
        "        f\"all {class_name}s\"\n",
        "        for class_name\n",
        "        in class_names\n",
        "    ]"
      ],
      "metadata": {
        "id": "9EdxE5d6Hv3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import supervision as sv\n",
        "\n",
        "# load image\n",
        "image = cv2.imread(SOURCE_IMAGE_PATH)\n",
        "\n",
        "# detect objects\n",
        "detections = grounding_dino_model.predict_with_classes(\n",
        "    image=image,\n",
        "    classes=enhance_class_name(class_names=CLASSES),\n",
        "    box_threshold=BOX_TRESHOLD,\n",
        "    text_threshold=TEXT_TRESHOLD\n",
        ")\n",
        "\n",
        "# annotate image with detections\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "labels = [\n",
        "    f\"{CLASSES[class_id]} {confidence:0.2f}\"\n",
        "    for _, _, confidence, class_id, _\n",
        "    in detections]\n",
        "annotated_frame = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.plot_image(annotated_frame, (16, 16))"
      ],
      "metadata": {
        "id": "LoD2bIptG-qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompting SAM with detected boxes"
      ],
      "metadata": {
        "id": "NDnK-8N3eFbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from segment_anything import SamPredictor\n",
        "\n",
        "\n",
        "def segment(sam_predictor: SamPredictor, image: np.ndarray, xyxy: np.ndarray) -> np.ndarray:\n",
        "    sam_predictor.set_image(image)\n",
        "    result_masks = []\n",
        "    for box in xyxy:\n",
        "        masks, scores, logits = sam_predictor.predict(\n",
        "            box=box,\n",
        "            multimask_output=True\n",
        "        )\n",
        "        index = np.argmax(scores)\n",
        "        result_masks.append(masks[index])\n",
        "    return np.array(result_masks)"
      ],
      "metadata": {
        "id": "1ZX6WsB9NMUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# convert detections to masks\n",
        "detections.mask = segment(\n",
        "    sam_predictor=sam_predictor,\n",
        "    image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n",
        "    xyxy=detections.xyxy\n",
        ")\n",
        "\n",
        "# annotate image with detections\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "mask_annotator = sv.MaskAnnotator()\n",
        "labels = [\n",
        "    f\"{CLASSES[class_id]} {confidence:0.2f}\"\n",
        "    for _, _, confidence, class_id, _\n",
        "    in detections]\n",
        "annotated_image = mask_annotator.annotate(scene=image.copy(), detections=detections)\n",
        "annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections, labels=labels)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.plot_image(annotated_image, (16, 16))"
      ],
      "metadata": {
        "id": "wy3UjxMHclSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "grid_size_dimension = math.ceil(math.sqrt(len(detections.mask)))\n",
        "\n",
        "titles = [\n",
        "    CLASSES[class_id]\n",
        "    for class_id\n",
        "    in detections.class_id\n",
        "]\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=detections.mask,\n",
        "    titles=titles,\n",
        "    grid_size=(grid_size_dimension, grid_size_dimension),\n",
        "    size=(16, 16)\n",
        ")"
      ],
      "metadata": {
        "id": "d4usQdQQ-4xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Dataset Mask Auto Annotation"
      ],
      "metadata": {
        "id": "AmFMQoEffdvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "IMAGES_DIRECTORY = os.path.join(HOME, 'data')\n",
        "IMAGES_EXTENSIONS = ['jpg', 'jpeg', 'png']\n",
        "\n",
        "CLASSES = ['car', 'dog', 'person', 'nose', 'chair', 'shoe', 'ear', 'coffee', 'backpack', 'cap']\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25"
      ],
      "metadata": {
        "id": "g-RAp9Vofo2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract labels from images"
      ],
      "metadata": {
        "id": "TsqhBBWVuSTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "images = {}\n",
        "annotations = {}\n",
        "\n",
        "image_paths = sv.list_files_with_extensions(\n",
        "    directory=IMAGES_DIRECTORY,\n",
        "    extensions=IMAGES_EXTENSIONS)\n",
        "\n",
        "for image_path in tqdm(image_paths):\n",
        "    image_name = image_path.name\n",
        "    image_path = str(image_path)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    detections = grounding_dino_model.predict_with_classes(\n",
        "        image=image,\n",
        "        classes=enhance_class_name(class_names=CLASSES),\n",
        "        box_threshold=BOX_TRESHOLD,\n",
        "        text_threshold=TEXT_TRESHOLD\n",
        "    )\n",
        "    detections = detections[detections.class_id != None]\n",
        "    detections.mask = segment(\n",
        "        sam_predictor=sam_predictor,\n",
        "        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n",
        "        xyxy=detections.xyxy\n",
        "    )\n",
        "    images[image_name] = image\n",
        "    annotations[image_name] = detections"
      ],
      "metadata": {
        "id": "N-F8nruxNbcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Before we save our `Detections` in Pascal VOC XML format, let's take a peek at the annotations we obtained. This step is optional, feel free to skip it."
      ],
      "metadata": {
        "id": "6a2BT8XsAzKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_images = []\n",
        "plot_titles = []\n",
        "\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "mask_annotator = sv.MaskAnnotator()\n",
        "\n",
        "for image_name, detections in annotations.items():\n",
        "    image = images[image_name]\n",
        "    plot_images.append(image)\n",
        "    plot_titles.append(image_name)\n",
        "\n",
        "    labels = [\n",
        "        f\"{CLASSES[class_id]} {confidence:0.2f}\"\n",
        "        for _, _, confidence, class_id, _\n",
        "        in detections]\n",
        "    annotated_image = mask_annotator.annotate(scene=image.copy(), detections=detections)\n",
        "    annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections, labels=labels)\n",
        "    plot_images.append(annotated_image)\n",
        "    title = \" \".join(set([\n",
        "        CLASSES[class_id]\n",
        "        for class_id\n",
        "        in detections.class_id\n",
        "    ]))\n",
        "    plot_titles.append(title)\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=plot_images,\n",
        "    titles=plot_titles,\n",
        "    grid_size=(len(annotations), 2),\n",
        "    size=(2 * 4, len(annotations) * 4)\n",
        ")"
      ],
      "metadata": {
        "id": "6d_4WDis946e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save labels in Pascal VOC XML\n",
        "\n",
        "Before uploading our annotations to Roboflow, we must first save them to our hard drive. To do this, we will use one of the latest `supervision` features (available with the `0.6.0` update üî•) - [dataset save](https://roboflow.github.io/supervision/dataset/core/)."
      ],
      "metadata": {
        "id": "traaYMeAuara"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ANNOTATIONS_DIRECTORY = os.path.join(HOME, 'annotations')\n",
        "\n",
        "MIN_IMAGE_AREA_PERCENTAGE = 0.002\n",
        "MAX_IMAGE_AREA_PERCENTAGE = 0.80\n",
        "APPROXIMATION_PERCENTAGE = 0.75"
      ],
      "metadata": {
        "id": "OELQUXECX9sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sv.Dataset(\n",
        "    classes=CLASSES,\n",
        "    images=images,\n",
        "    annotations=annotations\n",
        ").as_pascal_voc(\n",
        "    annotations_directory_path=ANNOTATIONS_DIRECTORY,\n",
        "    min_image_area_percentage=MIN_IMAGE_AREA_PERCENTAGE,\n",
        "    max_image_area_percentage=MAX_IMAGE_AREA_PERCENTAGE,\n",
        "    approximation_percentage=APPROXIMATION_PERCENTAGE\n",
        ")"
      ],
      "metadata": {
        "id": "IdRAslKSmzJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload annotations to Roboflow\n",
        "\n",
        "Now we are ready to upload our annotations to Roboflow using the API."
      ],
      "metadata": {
        "id": "ghegsbVLvNhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_NAME = \"auto-generated-dataset-7\"\n",
        "PROJECT_DESCRIPTION = \"auto-generated-dataset-7\""
      ],
      "metadata": {
        "id": "FFJ0TdiuvTH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import roboflow\n",
        "from roboflow import Roboflow\n",
        "\n",
        "roboflow.login()\n",
        "\n",
        "workspace = Roboflow().workspace()\n",
        "new_project = workspace.create_project(\n",
        "    project_name=PROJECT_NAME,\n",
        "    project_license=\"MIT\",\n",
        "    project_type=\"instance-segmentation\",\n",
        "    annotation=PROJECT_DESCRIPTION)"
      ],
      "metadata": {
        "id": "3rCOMnT8vRFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for image_path in tqdm(image_paths):\n",
        "    image_name = image_path.name\n",
        "    annotation_name = f\"{image_path.stem}.xml\"\n",
        "    image_path = str(image_path)\n",
        "    annotation_path = os.path.join(ANNOTATIONS_DIRECTORY, annotation_name)\n",
        "    new_project.upload(\n",
        "        image_path=image_path,\n",
        "        annotation_path=annotation_path,\n",
        "        split=\"train\",\n",
        "        is_prediction=True,\n",
        "        overwrite=True,\n",
        "        tag_names=[\"auto-annotated-with-grounded-sam\"],\n",
        "        batch_name=\"auto-annotated-with-grounded-sam\"\n",
        "    )"
      ],
      "metadata": {
        "id": "E9sb88IZvomH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert Object Detection to Instance Segmentation Dataset\n",
        "\n",
        "If you already have your dataset but it is annotated with bounding boxes, you can quickly and easily convert it to instance segmentation one. SAM allows using boxes as prompts. You can learn more about it from [this](https://blog.roboflow.com/how-to-use-segment-anything-model-sam/) tutorial."
      ],
      "metadata": {
        "id": "VAipLrT2Ki2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Object Detection Dataset from Roboflow\n",
        "\n",
        "To use the dataset in this tutorial make sure to download it in [Pascal VOC XML](https://roboflow.com/formats/pascal-voc-xml) format. We will use [BlueBerries dataset](https://universe.roboflow.com/inkyu-sa-e0c78/blueberries-u0e84) as example."
      ],
      "metadata": {
        "id": "Q8kCtsCZLcvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "\n",
        "import roboflow\n",
        "from roboflow import Roboflow\n",
        "\n",
        "roboflow.login()\n",
        "\n",
        "rf = Roboflow()\n",
        "\n",
        "project = rf.workspace(\"inkyu-sa-e0c78\").project(\"blueberries-u0e84\")\n",
        "dataset = project.version(1).download(\"voc\")"
      ],
      "metadata": {
        "id": "-9G8QyW0KkAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.location"
      ],
      "metadata": {
        "id": "hGCltTF9NXdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {dataset.location}"
      ],
      "metadata": {
        "id": "em2X4efFNbHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Visualize Object Detection Dataset with Supervision\n",
        "\n",
        "Learn more about Supervision Dataset support from our [documentation](https://roboflow.github.io/supervision/dataset/core/)."
      ],
      "metadata": {
        "id": "604mwY9yM2KR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "object_detection_dataset = sv.Dataset.from_pascal_voc(\n",
        "    images_directory_path=f\"{dataset.location}/train\",\n",
        "    annotations_directory_path=f\"{dataset.location}/train\"\n",
        ")"
      ],
      "metadata": {
        "id": "_cwtGbDUMywl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(9001)"
      ],
      "metadata": {
        "id": "FQCEuONKN1iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Rerun the cell below üëá to take a look at different image from the dataset."
      ],
      "metadata": {
        "id": "jhl2maP8O_jF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_names = list(object_detection_dataset.images.keys())\n",
        "image_name = random.choice(image_names)\n",
        "\n",
        "image = object_detection_dataset.images[image_name]\n",
        "detections = object_detection_dataset.annotations[image_name]\n",
        "\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "\n",
        "annotated_image = box_annotator.annotate(scene=image.copy(), detections=detections, skip_label=True)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.plot_image(annotated_image, (16, 16))"
      ],
      "metadata": {
        "id": "NfdAF-FSOLSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run SAM convert Boxes into Masks"
      ],
      "metadata": {
        "id": "qXd-tVNEP43n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "for image_name, image in tqdm(object_detection_dataset.images.items()):\n",
        "    detections = object_detection_dataset.annotations[image_name]\n",
        "    detections.mask = segment(\n",
        "        sam_predictor=sam_predictor,\n",
        "        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n",
        "        xyxy=detections.xyxy\n",
        "    )"
      ],
      "metadata": {
        "id": "SQ9dsieaQExn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Rerun the cell below üëá to take a look at different image from converted dataset."
      ],
      "metadata": {
        "id": "VtpM8peLRMnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_names = list(object_detection_dataset.images.keys())\n",
        "image_name = random.choice(image_names)\n",
        "\n",
        "image = object_detection_dataset.images[image_name]\n",
        "detections = object_detection_dataset.annotations[image_name]\n",
        "\n",
        "mask_annotator = sv.MaskAnnotator()\n",
        "\n",
        "annotated_image = mask_annotator.annotate(scene=image.copy(), detections=detections)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.plot_image(annotated_image, (16, 16))"
      ],
      "metadata": {
        "id": "D5MMYtW_RKXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save labels in Pascal VOC XML\n",
        "\n",
        "Before uploading our annotations to Roboflow, we must first save them to our hard drive. To do this, we will use one of the latest `supervision` features (available with the `0.6.0` update üî•) - [dataset save](https://roboflow.github.io/supervision/dataset/core/)."
      ],
      "metadata": {
        "id": "l_zhOj4ORx6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ANNOTATIONS_DIRECTORY = os.path.join(dataset.location, 'annotations')\n",
        "\n",
        "MIN_IMAGE_AREA_PERCENTAGE = 0.002\n",
        "MAX_IMAGE_AREA_PERCENTAGE = 0.80\n",
        "APPROXIMATION_PERCENTAGE = 0.75"
      ],
      "metadata": {
        "id": "cZ8fMRgYR8lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_detection_dataset.as_pascal_voc(\n",
        "    annotations_directory_path=ANNOTATIONS_DIRECTORY,\n",
        "    min_image_area_percentage=MIN_IMAGE_AREA_PERCENTAGE,\n",
        "    max_image_area_percentage=MAX_IMAGE_AREA_PERCENTAGE,\n",
        "    approximation_percentage=APPROXIMATION_PERCENTAGE\n",
        ")"
      ],
      "metadata": {
        "id": "RhiL4UEHSDra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload annotations to Roboflow\n",
        "\n",
        "Now we are ready to upload our annotations to Roboflow using the API."
      ],
      "metadata": {
        "id": "_HLp-Ns_Sszi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import roboflow\n",
        "from roboflow import Roboflow\n",
        "\n",
        "roboflow.login()\n",
        "\n",
        "workspace = Roboflow().workspace()\n",
        "new_project = workspace.create_project(\n",
        "    project_name=dataset.name,\n",
        "    project_license=\"MIT\",\n",
        "    project_type=\"instance-segmentation\",\n",
        "    annotation=f\"{dataset.name}-boxes-to-segmentations\")"
      ],
      "metadata": {
        "id": "3QoTKjLQS1r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "image_paths = sv.list_files_with_extensions(directory=f\"{dataset.location}/train\", extensions=[\"jpg\", \"jpeg\", \"png\"])\n",
        "for image_path in tqdm(image_paths):\n",
        "    image_name = image_path.name\n",
        "    annotation_name = f\"{image_path.stem}.xml\"\n",
        "    image_path = str(image_path)\n",
        "    annotation_path = os.path.join(ANNOTATIONS_DIRECTORY, annotation_name)\n",
        "    new_project.upload(\n",
        "        image_path=image_path,\n",
        "        annotation_path=annotation_path,\n",
        "        split=\"train\",\n",
        "        is_prediction=True,\n",
        "        overwrite=True,\n",
        "        tag_names=[\"auto-annotated-with-grounded-sam\"],\n",
        "        batch_name=\"auto-annotated-with-grounded-sam\"\n",
        "    )"
      ],
      "metadata": {
        "id": "pf6IukfRTCxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next steps\n",
        "\n",
        "- If you liked the tutorial please leave ‚≠ê in our [supervision](https://github.com/roboflow/supervision) and [notebooks](https://github.com/roboflow/notebooks) repos.\n",
        "- Review your dataset in Roboflow, correct any errors Grounding DINO / SAM made and [train real-time model](https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset/).\n",
        "- [SAM is built into Roboflow annotation tool as well](https://youtu.be/NAL2xcmMJiQ). If you need to refine the masks you can do it interactively in the browser."
      ],
      "metadata": {
        "id": "k2gPWTRjRdZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèÜ Congratulations\n",
        "\n",
        "### Learning Resources\n",
        "\n",
        "Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n",
        "\n",
        "- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.\n",
        "- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n",
        "- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n",
        "- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n",
        "\n",
        "### Convert data formats\n",
        "\n",
        "Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.\n",
        "\n",
        "### Connect computer vision to your project logic\n",
        "\n",
        "[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections."
      ],
      "metadata": {
        "id": "Yf3Hml73tRCT"
      }
    }
  ]
}