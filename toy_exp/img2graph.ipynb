{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对象图的构建\n",
    "此notebook是Zhou Shi-Wei学位论文开题报告的toy实验：\n",
    "将会从一张分割图中按照分割区域分别提取深度特征以及边关系的空间特征，以此构成图结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# Read the image using PIL\n",
    "seg_mask_path = '/data/datasets/mvtec_loco/MVTec_LOCO_AD_seg_all/breakfast_box/train/good/000.png'\n",
    "seg_mask = Image.open(seg_mask_path)\n",
    "# Display the image\n",
    "plt.imshow(seg_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 展示分割效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seg_array = np.array(seg_mask)\n",
    "#  读取不同颜色的图像\n",
    "unique_values = np.unique(seg_array)\n",
    "non_zero_values = unique_values[unique_values > 0]  # 将黑色排除\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(non_zero_values), figsize=(15, 5))\n",
    "\n",
    "# 迭代读取每一个非黑色区域\n",
    "for i, value in enumerate(non_zero_values):\n",
    "    # Create a mask where the current segment is preserved and everything else is set to 0\n",
    "    mask = np.zeros_like(seg_array)\n",
    "    mask[seg_array == value] = value\n",
    "    \n",
    "    # Create a PIL Image from the mask, using the same palette as the original image\n",
    "    mask_img = Image.fromarray(mask, mode='P')\n",
    "    mask_img.putpalette(seg_mask.getpalette())\n",
    "    \n",
    "    # Display the mask\n",
    "    axes[i].imshow(mask_img)\n",
    "    axes[i].set_title(f'Object {i+1} (Value: {value})')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载原始图像\n",
    "orig_img_path = '/data/datasets/mvtec_loco/orig_512/breakfast_box/train/good/000.png'\n",
    "orig_img = Image.open(orig_img_path)\n",
    "orig_img_array = np.array(orig_img)\n",
    "\n",
    "# Create a figure to display the original image and the masked regions\n",
    "fig_overlay, axes_overlay = plt.subplots(1, len(non_zero_values)+1, figsize=(20, 6))\n",
    "\n",
    "# Display the original image in the first subplot\n",
    "axes_overlay[0].imshow(orig_img)\n",
    "axes_overlay[0].set_title('Original Image')\n",
    "axes_overlay[0].axis('off')\n",
    "\n",
    "# Create overlays for each unique segmentation value\n",
    "for i, value in enumerate(non_zero_values):\n",
    "    # Create a mask for the current segment\n",
    "    mask = np.zeros_like(seg_array, dtype=bool)\n",
    "    mask[seg_array == value] = True\n",
    "    \n",
    "    # Create a copy of the original image\n",
    "    overlay_img = orig_img_array.copy()\n",
    "    \n",
    "    # Add a colored overlay to highlight the masked region\n",
    "    # Choose a different color for each segment\n",
    "    color = plt.cm.rainbow(i/len(non_zero_values))[:3]  # RGB color from rainbow colormap\n",
    "    \n",
    "    # Only show the masked region, everything else is black\n",
    "    overlay_img = np.zeros_like(orig_img_array)  # Create a black image\n",
    "    overlay_img[mask] = orig_img_array[mask]  # Copy only the masked region from original\n",
    "    \n",
    "    # Display the overlay\n",
    "    axes_overlay[i+1].imshow(overlay_img)\n",
    "    axes_overlay[i+1].set_title(f'Object {i+1} (Value: {value})')\n",
    "    axes_overlay[i+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取深度特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1]) # * 移除模型的分类头得到最后一层的特征\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Dictionary to store features for each object\n",
    "object_features = {}\n",
    "\n",
    "for i, value in enumerate(non_zero_values):\n",
    "    # Create mask for current object\n",
    "    mask = seg_array == value\n",
    "    \n",
    "    obj_img = orig_img_array.copy()\n",
    "    obj_img[~mask] = [0, 0, 0]  # * 背景全黑\n",
    "    \n",
    "    # TODO 按照mask区域对每个对象进行裁剪，即把所有的mask区域裁剪出来\n",
    "\n",
    "    obj_pil = Image.fromarray(obj_img)  # * 使用PIL转换图像\n",
    "    # Preprocess the image\n",
    "    input_tensor = preprocess(obj_pil)\n",
    "    input_batch = input_tensor.unsqueeze(0).to(device)  # * 添加batch维度并移动到GPU\n",
    "    with torch.no_grad():\n",
    "        features = model(input_batch)\n",
    "    \n",
    "    # Reshape and convert to numpy\n",
    "    features = features.squeeze().cpu().numpy() # * 去掉batch维度并移动到CPU\n",
    "    \n",
    "    # Store the features\n",
    "    object_features[f\"Object_{i+1}_Value_{value}\"] = features\n",
    "    \n",
    "    print(f\"Extracted features for Object {i+1} (Value: {value}), shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取空间特征（边关系）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * 计算空间关系\n",
    "spatial_relations = {}\n",
    "for i, value_i in enumerate(non_zero_values):\n",
    "    mask_i = seg_array == value_i\n",
    "    y_i, x_i = np.where(mask_i)\n",
    "    center_i = (np.mean(y_i), np.mean(x_i))\n",
    "    \n",
    "    for j, value_j in enumerate(non_zero_values):\n",
    "        if i != j:\n",
    "            mask_j = seg_array == value_j\n",
    "            y_j, x_j = np.where(mask_j)\n",
    "            center_j = (np.mean(y_j), np.mean(x_j))\n",
    "            \n",
    "            distance = np.sqrt((center_i[0] - center_j[0])**2 + (center_i[1] - center_j[1])**2)\n",
    "            direction = (center_j[0] - center_i[0], center_j[1] - center_i[1])\n",
    "            spatial_relations[f\"Object_{i+1}_to_Object_{j+1}\"] = {\n",
    "                \"distance\": distance,\n",
    "                \"direction\": direction\n",
    "            }\n",
    "            # # Print the spatial relationships for each pair\n",
    "            # print(f\"Object_{i+1} to Object_{j+1}:\")\n",
    "            # print(f\"  - Distance: {distance:.2f} pixels\")\n",
    "            # print(f\"  - Direction vector: {direction}\")\n",
    "            # print()\n",
    "\n",
    "print(\"Feature extraction and spatial relationship computation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建Graph结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Create a networkx graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with features\n",
    "for i, value in enumerate(non_zero_values):\n",
    "    node_id = i + 1  # Node IDs: 1, 2, 3, 4, 5, 6\n",
    "    feature_key = f\"Object_{node_id}_Value_{value}\"\n",
    "    G.add_node(node_id, features=object_features[feature_key], value=int(value))\n",
    "\n",
    "# Add edges with attributes (fully connected graph)\n",
    "for i in range(1, len(non_zero_values) + 1):\n",
    "    for j in range(i+1, len(non_zero_values) + 1):  # Start from i+1 to avoid duplicate edges\n",
    "        relation_key = f\"Object_{i}_to_Object_{j}\"\n",
    "        distance = spatial_relations[relation_key][\"distance\"]\n",
    "        direction = spatial_relations[relation_key][\"direction\"]\n",
    "        G.add_edge(i, j, distance=float(distance), direction=direction)\n",
    "\n",
    "# Convert to PyTorch Geometric format for deep learning\n",
    "node_features = []\n",
    "for i in range(1, len(non_zero_values) + 1):\n",
    "    feature_key = f\"Object_{i}_Value_{non_zero_values[i-1]}\"\n",
    "    node_features.append(object_features[feature_key])\n",
    "\n",
    "# Stack node features\n",
    "x = torch.tensor(np.stack(node_features), dtype=torch.float)\n",
    "\n",
    "# Create edge indices and attributes\n",
    "edge_index = []\n",
    "edge_attr = []\n",
    "for i in range(1, len(non_zero_values) + 1):\n",
    "    for j in range(1, len(non_zero_values) + 1):\n",
    "        if i != j:\n",
    "            edge_index.append([i-1, j-1])  # 0-based indexing\n",
    "            relation_key = f\"Object_{i}_to_Object_{j}\"\n",
    "            distance = spatial_relations[relation_key][\"distance\"]\n",
    "            direction_x, direction_y = spatial_relations[relation_key][\"direction\"]\n",
    "            edge_attr.append([float(distance), float(direction_x), float(direction_y)])\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()  # 2xE format\n",
    "edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "# Create PyG data object\n",
    "graph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=500, font_weight='bold')\n",
    "edge_labels = {(i, j): f\"{G.edges[i, j]['distance']:.1f}\" for i, j in G.edges()}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "plt.title(\"Object Graph with Spatial Relations\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "print(f\"PyTorch Geometric Data: {graph_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GATv2Conv, GCNConv\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a GNN model that supports edge attributes\n",
    "class EdgeAttrGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, edge_dim):\n",
    "        super(EdgeAttrGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GATv2Conv(hidden_channels, hidden_channels, edge_dim=edge_dim)\n",
    "        self.conv3 = GATv2Conv(hidden_channels, out_channels, edge_dim=edge_dim)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index, edge_attr=edge_attr)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv3(x, edge_index, edge_attr=edge_attr)\n",
    "        return x\n",
    "\n",
    "# Initialize model parameters\n",
    "in_channels = graph_data.x.size(1)  # 2048\n",
    "hidden_channels = 64\n",
    "out_channels = 16  # Can be adjusted based on your task\n",
    "edge_dim = graph_data.edge_attr.size(1)  # 3 (distance, direction_x, direction_y)\n",
    "\n",
    "# Create model and move to the same device as the data\n",
    "model = EdgeAttrGNN(in_channels, hidden_channels, out_channels, edge_dim).to(device)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(graph_data.x.to(device), \n",
    "                   graph_data.edge_index.to(device), \n",
    "                   graph_data.edge_attr.to(device))\n",
    "\n",
    "print(f\"Input shape: {graph_data.x.shape}\")\n",
    "print(f\"Edge index shape: {graph_data.edge_index.shape}\")\n",
    "print(f\"Edge attributes shape: {graph_data.edge_attr.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"Forward pass successful!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
